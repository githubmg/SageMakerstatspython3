{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Build an image that can do training and inference in SageMaker\r\n",
      "# This is a Python 2 image that uses the nginx, gunicorn, flask stack\r\n",
      "# for serving inferences in a stable way.\r\n",
      "\r\n",
      "FROM ubuntu:16.04\r\n",
      "\r\n",
      "MAINTAINER Amazon AI <sage-learner@amazon.com>\r\n",
      "\r\n",
      "RUN apt-get -y update && apt-get install -y --no-install-recommends \\\r\n",
      "         wget \\\r\n",
      "         python \\\r\n",
      "         python3.5 \\\r\n",
      "         nginx \\\r\n",
      "         ca-certificates \\\r\n",
      "         libgcc-5-dev \\\r\n",
      "    && rm -rf /var/lib/apt/lists/*\r\n",
      "\r\n",
      "\r\n",
      "# Symlink /usr/bin/python to the python version we're building for.\r\n",
      "RUN rm /usr/bin/python && ln -s /usr/bin/python3.5 /usr/bin/python\r\n",
      "\r\n",
      "# Here we get all python packages.\r\n",
      "# There's substantial overlap between scipy and numpy that we eliminate by\r\n",
      "# linking them together. Likewise, pip leaves the install caches populated which uses\r\n",
      "# a significant amount of space. These optimizations save a fair amount of space in the\r\n",
      "# image, which reduces start up time.\r\n",
      "RUN wget https://bootstrap.pypa.io/3.3/get-pip.py && python3.5 get-pip.py && \\\r\n",
      " pip3 install numpy scipy scikit-learn statsmodels pandas flask gevent gunicorn && \\\r\n",
      " (cd /usr/local/lib/python3.5/dist-packages/scipy/.libs; rm *; ln ../../numpy/.libs/* .) && \\\r\n",
      " rm -rf /root/.cache\r\n",
      "\r\n",
      "# Set some environment variables. PYTHONUNBUFFERED keeps Python from buffering our standard\r\n",
      "# output stream, which means that logs can be delivered to the user quickly. PYTHONDONTWRITEBYTECODE\r\n",
      "# keeps Python from writing the .pyc files which are unnecessary in this case. We also update\r\n",
      "# PATH so that the train and serve programs are found when the container is invoked.\r\n",
      "\r\n",
      "ENV PYTHONUNBUFFERED=TRUE\r\n",
      "ENV PYTHONDONTWRITEBYTECODE=TRUE\r\n",
      "ENV PATH=\"/opt/program:${PATH}\"\r\n",
      "\r\n",
      "# Set up the program in the image\r\n",
      "COPY gamma-migration /opt/program\r\n",
      "WORKDIR /opt/program\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!cat container/Dockerfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building and registering the container\n",
    "\n",
    "The following shell code shows how to build the container image using `docker build` and push the container image to ECR using `docker push`. This code is also available as the shell script `container/build-and-push.sh`, which you can run as `build-and-push.sh decision_trees_sample` to build the image `decision_trees_sample`. \n",
    "\n",
    "This code looks for an ECR repository in the account you're using and the current default region (if you're using a SageMaker notebook instance, this will be the region where the notebook instance was created). If the repository doesn't exist, the script will create it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Login Succeeded\n",
      "Sending build context to Docker daemon  21.83MB\r",
      "\r\n",
      "Step 1/10 : FROM ubuntu:16.04\n",
      " ---> 9361ce633ff1\n",
      "Step 2/10 : MAINTAINER Amazon AI <sage-learner@amazon.com>\n",
      " ---> Using cache\n",
      " ---> 7a6b1f6328eb\n",
      "Step 3/10 : RUN apt-get -y update && apt-get install -y --no-install-recommends          wget          python          python3.5          nginx          ca-certificates          libgcc-5-dev     && rm -rf /var/lib/apt/lists/*\n",
      " ---> Using cache\n",
      " ---> cc27d861e6c8\n",
      "Step 4/10 : RUN rm /usr/bin/python && ln -s /usr/bin/python3.5 /usr/bin/python\n",
      " ---> Using cache\n",
      " ---> f573dddf4eb5\n",
      "Step 5/10 : RUN wget https://bootstrap.pypa.io/3.3/get-pip.py && python3.5 get-pip.py &&  pip3 install numpy scipy scikit-learn statsmodels pandas flask gevent gunicorn &&  (cd /usr/local/lib/python3.5/dist-packages/scipy/.libs; rm *; ln ../../numpy/.libs/* .) &&  rm -rf /root/.cache\n",
      " ---> Using cache\n",
      " ---> 49314852a51c\n",
      "Step 6/10 : ENV PYTHONUNBUFFERED=TRUE\n",
      " ---> Using cache\n",
      " ---> 0eabe1e6cac3\n",
      "Step 7/10 : ENV PYTHONDONTWRITEBYTECODE=TRUE\n",
      " ---> Using cache\n",
      " ---> e40fc00b3798\n",
      "Step 8/10 : ENV PATH=\"/opt/program:${PATH}\"\n",
      " ---> Using cache\n",
      " ---> 9671b6694ae8\n",
      "Step 9/10 : COPY gamma-migration /opt/program\n",
      " ---> 8866ab0970d5\n",
      "Step 10/10 : WORKDIR /opt/program\n",
      " ---> Running in a0491a41a639\n",
      "Removing intermediate container a0491a41a639\n",
      " ---> 6d923c8f8e87\n",
      "Successfully built 6d923c8f8e87\n",
      "Successfully tagged gamma-migration:latest\n",
      "The push refers to repository [452432741922.dkr.ecr.us-east-1.amazonaws.com/gamma-migration]\n",
      "642b9717e1c4: Preparing\n",
      "82bb22437edd: Preparing\n",
      "ba9d45d358d7: Preparing\n",
      "d91b8f752bf1: Preparing\n",
      "297fd071ca2f: Preparing\n",
      "2f0d1e8214b2: Preparing\n",
      "7dd604ffa87f: Preparing\n",
      "aa54c2bc1229: Preparing\n",
      "2f0d1e8214b2: Waiting\n",
      "7dd604ffa87f: Waiting\n",
      "aa54c2bc1229: Waiting\n",
      "ba9d45d358d7: Layer already exists\n",
      "d91b8f752bf1: Layer already exists\n",
      "297fd071ca2f: Layer already exists\n",
      "82bb22437edd: Layer already exists\n",
      "2f0d1e8214b2: Layer already exists\n",
      "7dd604ffa87f: Layer already exists\n",
      "aa54c2bc1229: Layer already exists\n",
      "642b9717e1c4: Pushed\n",
      "latest: digest: sha256:4f0ec7cf27cf505297cc908803a922ff75761222a1f21abf1cbd46b14995ffc5 size: 1989\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING! Using --password via the CLI is insecure. Use --password-stdin.\n",
      "WARNING! Your password will be stored unencrypted in /home/ec2-user/.docker/config.json.\n",
      "Configure a credential helper to remove this warning. See\n",
      "https://docs.docker.com/engine/reference/commandline/login/#credentials-store\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "\n",
    "# The name of our algorithm\n",
    "algorithm_name=gamma-migration\n",
    "\n",
    "cd container\n",
    "\n",
    "chmod +x gamma-migration/train\n",
    "chmod +x gamma-migration/serve\n",
    "\n",
    "account=$(aws sts get-caller-identity --query Account --output text)\n",
    "\n",
    "# Get the region defined in the current configuration (default to us-west-2 if none defined)\n",
    "region=$(aws configure get region)\n",
    "region=${region:-us-west-2}\n",
    "\n",
    "fullname=\"${account}.dkr.ecr.${region}.amazonaws.com/${algorithm_name}:latest\"\n",
    "\n",
    "# If the repository doesn't exist in ECR, create it.\n",
    "aws ecr describe-repositories --repository-names \"${algorithm_name}\" > /dev/null 2>&1\n",
    "\n",
    "if [ $? -ne 0 ]\n",
    "then\n",
    "    aws ecr create-repository --repository-name \"${algorithm_name}\" > /dev/null\n",
    "fi\n",
    "\n",
    "# Get the login command from ECR and execute it directly\n",
    "$(aws ecr get-login --region ${region} --no-include-email)\n",
    "\n",
    "# Build the docker image locally with the image name and then push it to ECR\n",
    "# with the full name.\n",
    "\n",
    "docker build  -t ${algorithm_name} .\n",
    "docker tag ${algorithm_name} ${fullname}\n",
    "\n",
    "docker push ${fullname}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing your algorithm on your local machine or on an Amazon SageMaker notebook instance\n",
    "\n",
    "While you're first packaging an algorithm use with Amazon SageMaker, you probably want to test it yourself to make sure it's working right. In the directory `container/local_test`, there is a framework for doing this. It includes three shell scripts for running and using the container and a directory structure that mimics the one outlined above.\n",
    "\n",
    "The scripts are:\n",
    "\n",
    "* `train_local.sh`: Run this with the name of the image and it will run training on the local tree. You'll want to modify the directory `test_dir/input/data/...` to be set up with the correct channels and data for your algorithm. Also, you'll want to modify the file `input/config/hyperparameters.json` to have the hyperparameter settings that you want to test (as strings).\n",
    "* `serve_local.sh`: Run this with the name of the image once you've trained the model and it should serve the model. It will run and wait for requests. Simply use the keyboard interrupt to stop it.\n",
    "* `predict.sh`: Run this with the name of a payload file and (optionally) the HTTP content type you want. The content type will default to `text/csv`. For example, you can run `$ ./predict.sh payload.csv text/csv`.\n",
    "\n",
    "The directories as shipped are set up to test the decision trees sample algorithm presented here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Using your Algorithm in Amazon SageMaker\n",
    "\n",
    "Once you have your container packaged, you can use it to train models and use the model for hosting or batch transforms. Let's do that with the algorithm we made above.\n",
    "\n",
    "## Set up the environment\n",
    "\n",
    "Here we specify a bucket to use and the role that will be used for working with SageMaker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# S3 prefix\n",
    "prefix = 'gamma-migration'\n",
    "\n",
    "# Define IAM role\n",
    "import boto3\n",
    "import re\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "role = get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the session\n",
    "\n",
    "The session remembers our connection parameters to SageMaker. We'll use it to perform all of our SageMaker operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker as sage\n",
    "from time import gmtime, strftime\n",
    "\n",
    "sess = sage.Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload the data for training\n",
    "\n",
    "When training large models with huge amounts of data, you'll typically use big data tools, like Amazon Athena, AWS Glue, or Amazon EMR, to create your data in S3. For the purposes of this example, we're using some the classic [Iris dataset](https://en.wikipedia.org/wiki/Iris_flower_data_set), which we have included. \n",
    "\n",
    "We can use use the tools provided by the SageMaker Python SDK to upload the data to a default bucket. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Created S3 bucket: sagemaker-us-east-1-452432741922\n"
     ]
    }
   ],
   "source": [
    "WORK_DIRECTORY = 'data'\n",
    "\n",
    "data_location = sess.upload_data(WORK_DIRECTORY, key_prefix=prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://sagemaker-us-east-1-452432741922/gamma-migration'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_location"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an estimator and fit the model\n",
    "\n",
    "In order to use SageMaker to fit our algorithm, we'll create an `Estimator` that defines how to use the container to train. This includes the configuration we need to invoke SageMaker training:\n",
    "\n",
    "* The __container name__. This is constructed as in the shell commands above.\n",
    "* The __role__. As defined above.\n",
    "* The __instance count__ which is the number of machines to use for training.\n",
    "* The __instance type__ which is the type of machine to use for training.\n",
    "* The __output path__ determines where the model artifact will be written.\n",
    "* The __session__ is the SageMaker session object that we defined above.\n",
    "\n",
    "Then we use fit() on the estimator to train against the data that we uploaded above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating training-job with name: gamma-migration-2019-04-01-18-03-12-104\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-04-01 18:03:12 Starting - Starting the training job...\n",
      "2019-04-01 18:03:18 Starting - Launching requested ML instances......\n",
      "2019-04-01 18:04:29 Starting - Preparing the instances for training......\n",
      "2019-04-01 18:05:34 Downloading - Downloading input data\n",
      "2019-04-01 18:05:34 Training - Downloading the training image..\n",
      "\u001b[31mStarting the training.\u001b[0m\n",
      "\u001b[31m['/opt/ml/input/data/training/train.csv']\u001b[0m\n",
      "\u001b[31mTraining complete.\u001b[0m\n",
      "\n",
      "2019-04-01 18:06:03 Uploading - Uploading generated training model\n",
      "2019-04-01 18:06:03 Completed - Training job completed\n",
      "Billable seconds: 38\n"
     ]
    }
   ],
   "source": [
    "account = sess.boto_session.client('sts').get_caller_identity()['Account']\n",
    "region = sess.boto_session.region_name\n",
    "image = '{}.dkr.ecr.{}.amazonaws.com/gamma-migration:latest'.format(account, region)\n",
    "\n",
    "tree = sage.estimator.Estimator(image,\n",
    "                       role, 1, 'ml.c4.2xlarge',\n",
    "                       output_path=\"s3://{}/output\".format(sess.default_bucket()),\n",
    "                       sagemaker_session=sess)\n",
    "\n",
    "tree.fit(data_location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hosting your model\n",
    "You can use a trained model to get real time predictions using HTTP endpoint. Follow these steps to walk you through the process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy the model\n",
    "\n",
    "Deploying the model to SageMaker hosting just requires a `deploy` call on the fitted model. This call takes an instance count, instance type, and optionally serializer and deserializer functions. These are used when the resulting predictor is created on the endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating model with name: gamma-migration-2019-04-01-18-13-58-175\n",
      "INFO:sagemaker:Creating endpoint with name gamma-migration-2019-04-01-18-03-12-104\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------!"
     ]
    }
   ],
   "source": [
    "from sagemaker.predictor import csv_serializer\n",
    "predictor = tree.deploy(1, 'ml.m4.xlarge', serializer=csv_serializer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose some data and use it for a prediction\n",
    "\n",
    "In order to do some predictions, we'll extract some of the data we used for training and do predictions against it. This is, of course, bad statistical practice, but a good way to see how the mechanism works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv(\"data/test.csv\")[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.to_csv('data/test_sample.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "client = boto3.client('sagemaker-runtime')\n",
    "\n",
    "endpoint_name = \"gamma-migration-2019-04-01-18-03-12-104\"                                       # Your endpoint name.\n",
    "content_type = \"text/csv\"                                        # The MIME type of the input data in the request body."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45.7 ms ± 4.14 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "response = client.invoke_endpoint(\n",
    "    EndpointName=endpoint_name, \n",
    "    ContentType=content_type,\n",
    "    Body=open('data/test_sample.csv', 'rb')\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ResponseMetadata': {'RequestId': 'c85931e8-2272-425c-93ae-b29284e82acf', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': 'c85931e8-2272-425c-93ae-b29284e82acf', 'x-amzn-invoked-production-variant': 'AllTraffic', 'date': 'Sun, 7 Apr 2019 15:05:51 GMT', 'content-type': 'text/csv; charset=utf-8', 'content-length': '1991'}, 'RetryAttempts': 0}, 'ContentType': 'text/csv; charset=utf-8', 'InvokedProductionVariant': 'AllTraffic', 'Body': <botocore.response.StreamingBody object at 0x7f0120dc7e48>}\n"
     ]
    }
   ],
   "source": [
    "response = client.invoke_endpoint(\n",
    "    EndpointName=endpoint_name, \n",
    "    ContentType=content_type,\n",
    "    Body=open('data/test_sample.csv', 'rb')\n",
    "    )\n",
    "\n",
    "print(response)                         # If model receives and updates the custom_attributes header \n",
    "                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'0.1132681406730016\\n0.07929910698279859\\n0.06768094107442993\\n0.07929910698279859\\n0.12894661830848228\\n0.04901697205745189\\n0.08065383969673699\\n0.0753356698083712\\n0.1071239082316859\\n0.0781579522050119\\n0.08534729984793873\\n0.11113706589299929\\n0.07817981346688647\\n0.09118231784968665\\n0.07245874080401164\\n0.1071239082316859\\n0.09998530179350963\\n0.08782380648056719\\n0.18071765699376435\\n0.13767613023405753\\n0.10918243495138684\\n0.06713266122136087\\n0.09637483415440823\\n0.12607879131239255\\n0.06969991280138747\\n0.06847406069620009\\n0.06969991280138747\\n0.059359030514617314\\n0.07741066336532196\\n0.08203286181057012\\n0.1407897503918773\\n0.054484032709875355\\n0.09118231784968665\\n0.10374010651499568\\n0.11333756954015345\\n0.10704356280890878\\n0.08203286181057012\\n0.08367535543525223\\n0.09637483415440823\\n0.09637483415440823\\n0.14014086757204394\\n0.054484032709875355\\n0.08373816100367742\\n0.09637483415440823\\n0.0753356698083712\\n0.0753356698083712\\n0.09998530179350963\\n0.12644305744135528\\n0.06847406069620009\\n0.08687522394250699\\n0.06198766921023605\\n0.10916301547281268\\n0.12607879131239255\\n0.09637483415440823\\n0.054484032709875355\\n0.08226541022358126\\n0.06969991280138747\\n0.12607879131239255\\n0.1071239082316859\\n0.06969991280138747\\n0.09637483415440823\\n0.10704356280890878\\n0.08065383969673699\\n0.09648809183145807\\n0.1071239082316859\\n0.10317822147810159\\n0.10918243495138684\\n0.09637483415440823\\n0.10918243495138684\\n0.06768094107442993\\n0.11113706589299929\\n0.07929910698279859\\n0.09637483415440823\\n0.09648809183145807\\n0.10523985898192705\\n0.07159422893551838\\n0.10704356280890878\\n0.10317822147810159\\n0.08658235186670714\\n0.10918243495138684\\n0.09637483415440823\\n0.0781579522050119\\n0.06198766921023605\\n0.09998530179350963\\n0.05352579096304825\\n0.08367535543525223\\n0.09454661666319521\\n0.10918243495138684\\n0.10506569230364866\\n0.05352579096304825\\n0.09269445862480286\\n0.08226541022358126\\n0.071237797622965\\n0.10704356280890878\\n0.08065383969673699\\n0.14014086757204394\\n0.08065383969673699\\n0.09637483415440823\\n0.09648809183145807\\n0.14014086757204394\\n'\n"
     ]
    }
   ],
   "source": [
    "print(response['Body'].read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional cleanup\n",
    "When you're done with the endpoint, you'll want to clean it up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.delete_endpoint('gamma-migration-2019-04-01-18-03-12-104')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
